---
title: Конкурентное выполнение запросов с помощью gather
date of creation: 2025-04-30T11:13:00
tags:
  - Python
  - Concurrency
  - Python/Concurrency
  - Developing
  - Developing/Python
read status: true
completion status: true
aliases:
  - asyncio.gather
  - python asyncio gather
  - gather в asyncio
---
# Конкурентное выполнение запросов с помощью gather
---
>[!tip]
Функция **`gather`** отлично подходит для множественных асинхронных запросов в отличие от **`asyncio.create_task`**.

Для конкурентного выполнения допускающих ожидание объектов широко используется функция **`asyncio.gather`**. Она принимает последовательность допускающих ожидание объектов и запускает их конкурентно всего в одной строке кода. Если среди объектов есть сопрограмма, то **gather** автоматически обертывает ее задачей, чтобы гарантировать конкурентное выполнение. ==Это значит, что не нужно отдельно обертывать все сопрограммы по отдельности с помощью функции== `asyncio.create_task`.

**`asyncio.gather`** возвращает объект, допускающий ожидание. Если использовать его в выражении **`await`**, то выполнение будет приостановлено, пока не завершатся все переданные объекты. А когда это произойдет, **`asyncio.gather`** вернет список результатов работы.

Мы можем воспользоваться этой функцией, чтобы конкурентно отправить любое число веб-запросов. Рассмотрим пример, где имеется 1000 запросов и мы хотим получить все коды состояния. Снабдим сопрограмму **`main`** декоратором **`@async_timed`**, чтобы знать, сколько прошло времени.

```python
import asyncio
import aiohttp
from aiohttp import ClientSession
from get_request_aiohttp import fetch_status
from async_timer import async_timed


@async_timed()
async def main():
    async with aiohttp.ClientSession() as session:
        urls = ["https://example.com" for _ in range(1000)]
        # Сгенерировать список сопрограмм для каждого запроса, который мы хотим отправить
        requests = [fetch_status(session, url) for url in urls]
        status_codes = await asyncio.gather(*requests)  # Ждать завершения всех запросов
        print(status_codes)

  
asyncio.run(main())

```

Здесь мы сначала генерируем список URL-адресов, для которых хотим получить код состояния; для простоты все адреса равны example. com. Затем берем этот список и вызываем **`fetch_status_code`**, чтобы получить список сопрограмм, который передадим gather. При этом все сопрограммы обертываются задачами и запускаются конкурентно. Во время выполнения на стандартном выводе будет напечатано 1000 сообщений, показывающих, что сопрограммы **`fetch_status_code`** запущены последовательно, но запросы выполняются конкурентно. Когда придут все результаты, мы увидим сообщение вида «`<function fetch_status_code at 0x10f3fe3a0> завершилась за 0.5453 с`». После того как содержимое всех запрошенных URL-адресов будет получено, начнут печататься коды состояния. Продолжительность всего процесса зависит от скорости интернет-подключения и быстродействия компьютера, обычно скрипт завершается за 500–600 мс.

А если сравнить с синхронным выполнением? Функцию **`main`** легко модифицировать, так чтобы она ждала **`fetch_status_code`** с помощью **`await`**, т. е. блокировала выполнение при каждом запросе. *Так мы заставим работать ее синхронно.*

```python
@async.timed()
async def main():
    async with aiohttp.ClienSession() as session:
        urls = ["http://example.com" for _ in range(1000)]
        status_codes = [await fetch_status_code(session, url)
                        for url in urls]
        print(status_codes)

```

Теперь программа будет работать гораздо дольше. Кроме того, вместо 1000 сообщений «**`выполняется function fetch_status_code`**», за которыми следует 1000 сообщений «**`function fetch_status_code завершилась`**», мы увидим следующую пару строк для каждого запроса:

```shell
выполняется <function delay at 0x10d95b310> с аргументами (3,) {} <function delay at 0x10d95b310> завершилась за 0.01884 с
```

Это означает, что запросы следуют друг за другом и каждый начинается только после того, как предыдущий вызов **`fetch_status_code`** завершился. Так насколько же это медленнее асинхронной версии? Зависит от интернет-подключения и машины, на которой запущена программа, но приблизительно выполнение занимает 18 с. ==То есть асинхронная версия работает в 33 раза быстрее. Впечатляет!==

Стоит отметить, что порядок поступления результатов для переданных объектов, допускающих ожидание, не детерминирован. Например, если передать gather сопрограммы a и b именно в таком порядке, то b может завершиться раньше, чем a. Но приятная особенность gather заключается в том, что, независимо от порядка завершения допускающих ожидание объектов, результаты гарантированно будут возвращены в том порядке, в каком объекты передавались. Продемонстрируем это в описанном ранее сценарии использования функции **`delay`**.

```python
import asyncio
from util import delay


async def main():
    results = await asyncio.gather(delay(3), delay(1))
    print(results)

asyncio.run(main())

```

Здесь мы передали **`gather`** две сопрограммы. Первой для завершения требуется 3 с, второй – одна. Можно ожидать, что результатом будет список **`[1, 3]`**, потому что односекундная сопрограмма завершается раньше трехсекундной. Но на самом деле возвращается **`[3, 1]`** в том порядке, в каком сопрограммы передавались. ***Функция `gather` гарантирует детерминированный порядок результатов, несмотря на недетерминированность их получения.*** На внутреннем уровне **`gather`** использует для этой цели специальную реализацию **`future`**. Интересующемуся читателю предлагается заглянуть в исходный код **`gather`**, этот поучительный опыт поможет осознать, как много API **`asyncio`** построено с использованием будущих объектов.
